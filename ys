[1mdiff --git a/Python/image-analysis/image-analysis.py b/Python/image-analysis/image-analysis.py[m
[1mindex a2c8c15..8cc8a55 100644[m
[1m--- a/Python/image-analysis/image-analysis.py[m
[1m+++ b/Python/image-analysis/image-analysis.py[m
[36m@@ -7,6 +7,10 @@[m [mfrom azure.core.exceptions import HttpResponseError[m
 import requests[m
 [m
 # Import namespaces[m
[32m+[m[32m# import namespaces[m
[32m+[m[32mfrom azure.ai.vision.imageanalysis import ImageAnalysisClient[m
[32m+[m[32mfrom azure.ai.vision.imageanalysis.models import VisualFeatures[m
[32m+[m[32mfrom azure.core.credentials import AzureKeyCredential[m
 [m
 [m
 def main():[m
[36m@@ -27,11 +31,15 @@[m [mdef main():[m
             image_data = f.read()[m
 [m
         # Authenticate Azure AI Vision client[m
[32m+[m[32m        # Authenticate Azure AI Vision client[m
[32m+[m[32m        cv_client = ImageAnalysisClient([m
[32m+[m[32m            endpoint=ai_endpoint,[m
[32m+[m[32m            credential=AzureKeyCredential(ai_key)[m
[32m+[m[32m        )[m
 [m
[31m-        [m
         # Analyze image[m
         AnalyzeImage(image_file, image_data, cv_client)[m
[31m-        [m
[32m+[m
         # Background removal[m
         BackgroundForeground(ai_endpoint, ai_key, image_file)[m
 [m
[36m@@ -44,7 +52,16 @@[m [mdef AnalyzeImage(image_filename, image_data, cv_client):[m
 [m
     try:[m
         # Get result with specified features to be retrieved[m
[31m-        [m
[32m+[m[32m        # Get result with specified features to be retrieved[m
[32m+[m[32m        result = cv_client.analyze([m
[32m+[m[32m            image_data=image_data,[m
[32m+[m[32m            visual_features=[[m
[32m+[m[32m                VisualFeatures.CAPTION,[m
[32m+[m[32m                VisualFeatures.DENSE_CAPTIONS,[m
[32m+[m[32m                VisualFeatures.TAGS,[m
[32m+[m[32m                VisualFeatures.OBJECTS,[m
[32m+[m[32m                VisualFeatures.PEOPLE],[m
[32m+[m[32m        )[m
 [m
     except HttpResponseError as e:[m
         print(f"Status code: {e.status_code}")[m
[36m@@ -52,15 +69,111 @@[m [mdef AnalyzeImage(image_filename, image_data, cv_client):[m
         print(f"Message: {e.error.message}")[m
 [m
     # Display analysis results[m
[31m-    [m
[32m+[m[32m    # Display analysis results[m
[32m+[m[32m    # Get image captions[m
[32m+[m[32m    if result.caption is not None:[m
[32m+[m[32m        print("\nCaption:")[m
[32m+[m[32m        print(" Caption: '{}' (confidence: {:.2f}%)".format(result.caption.text, result.caption.confidence * 100))[m
[32m+[m
[32m+[m[32m    # Get image dense captions[m
[32m+[m[32m    if result.dense_captions is not None:[m
[32m+[m[32m        print("\nDense Captions:")[m
[32m+[m[32m        for caption in result.dense_captions.list:[m
[32m+[m[32m            print(" Caption: '{}' (confidence: {:.2f}%)".format(caption.text, caption.confidence * 100))[m
[32m+[m
[32m+[m[32m    # Get image tags[m
[32m+[m[32m    # Get image tags[m
[32m+[m[32m    if result.tags is not None:[m
[32m+[m[32m        print("\nTags:")[m
[32m+[m[32m        for tag in result.tags.list:[m
[32m+[m[32m            print(" Tag: '{}' (confidence: {:.2f}%)".format(tag.name, tag.confidence * 100))[m
[32m+[m
[32m+[m[32m    # Get objects in the image[m
[32m+[m[32m    # Get objects in the image[m
[32m+[m[32m    if result.objects is not None:[m
[32m+[m[32m        print("\nObjects in image:")[m
[32m+[m
[32m+[m[32m        # Prepare image for drawing[m
[32m+[m[32m        image = Image.open(image_filename)[m
[32m+[m[32m        fig = plt.figure(figsize=(image.width/100, image.height/100))[m
[32m+[m[32m        plt.axis('off')[m
[32m+[m[32m        draw = ImageDraw.Draw(image)[m
[32m+[m[32m        color = 'cyan'[m
[32m+[m
[32m+[m[32m        for detected_object in result.objects.list:[m
[32m+[m[32m            # Print object name[m
[32m+[m[32m            print(" {} (confidence: {:.2f}%)".format(detected_object.tags[0].name, detected_object.tags[0].confidence * 100))[m
[32m+[m
[32m+[m[32m            # Draw object bounding box[m
[32m+[m[32m            r = detected_object.bounding_box[m
[32m+[m[32m            bounding_box = ((r.x, r.y), (r.x + r.width, r.y + r.height))[m
[32m+[m[32m            draw.rectangle(bounding_box, outline=color, width=3)[m
[32m+[m[32m            plt.annotate(detected_object.tags[0].name,(r.x, r.y), backgroundcolor=color)[m
[32m+[m
[32m+[m[32m        # Save annotated image[m
[32m+[m[32m        plt.imshow(image)[m
[32m+[m[32m        plt.tight_layout(pad=0)[m
[32m+[m[32m        outputfile = 'objects.jpg'[m
[32m+[m[32m        fig.savefig(outputfile)[m
[32m+[m[32m        print('  Results saved in', outputfile)[m
[32m+[m
[32m+[m[32m    # Get people in the image[m
[32m+[m[32m    # Get people in the image[m
[32m+[m[32m    if result.people is not None:[m
[32m+[m[32m        print("\nPeople in image:")[m
[32m+[m
[32m+[m[32m        # Prepare image for drawing[m
[32m+[m[32m        image = Image.open(image_filename)[m
[32m+[m[32m        fig = plt.figure(figsize=(image.width/100, image.height/100))[m
[32m+[m[32m        plt.axis('off')[m
[32m+[m[32m        draw = ImageDraw.Draw(image)[m
[32m+[m[32m        color = 'cyan'[m
[32m+[m
[32m+[m[32m        for detected_people in result.people.list:[m
[32m+[m[32m            # Draw object bounding box[m
[32m+[m[32m            r = detected_people.bounding_box[m
[32m+[m[32m            bounding_box = ((r.x, r.y), (r.x + r.width, r.y + r.height))[m
[32m+[m[32m            draw.rectangle(bounding_box, outline=color, width=3)[m
[32m+[m
[32m+[m[32m            # Return the confidence of the person detected[m
[32m+[m[32m            print(" {} (confidence: {:.2f}%)".format(detected_people.bounding_box, detected_people.confidence * 100))[m
[32m+[m
[32m+[m[32m        # Save annotated image[m
[32m+[m[32m        plt.imshow(image)[m
[32m+[m[32m        plt.tight_layout(pad=0)[m
[32m+[m[32m        outputfile = 'people.jpg'[m
[32m+[m[32m        fig.savefig(outputfile)[m
[32m+[m[32m        print('  Results saved in', outputfile)[m
[32m+[m
 [m
 def BackgroundForeground(endpoint, key, image_file):[m
     # Define the API version and mode[m
     api_version = "2023-02-01-preview"[m
[31m-    mode="backgroundRemoval" # Can be "foregroundMatting" or "backgroundRemoval"[m
[31m-    [m
[32m+[m[32m    mode = "backgroundRemoval" # Can be "foregroundMatting" or "backgroundRemoval"[m
[32m+[m
[32m+[m[32m    # Remove the background from the image or generate a foreground matte[m
     # Remove the background from the image or generate a foreground matte[m
[31m-    [m
[32m+[m[32m    print('\nRemoving background from image...')[m
[32m+[m
[32m+[m[32m    url = "{}computervision/imageanalysis:segment?api-version={}&mode={}".format(endpoint, api_version, mode)[m
[32m+[m
[32m+[m[32m    headers= {[m
[32m+[m[32m        "Ocp-Apim-Subscription-Key": key,[m
[32m+[m[32m        "Content-Type": "application/json"[m
[32m+[m[32m    }[m
[32m+[m
[32m+[m[32m    image_url="https://github.com/MicrosoftLearning/mslearn-ai-vision/blob/main/Labfiles/01-analyze-images/Python/image-analysis/{}?raw=true".format(image_file)[m
[32m+[m
[32m+[m[32m    body = {[m
[32m+[m[32m        "url": image_url,[m
[32m+[m[32m    }[m
[32m+[m
[32m+[m[32m    response = requests.post(url, headers=headers, json=body)[m
[32m+[m
[32m+[m[32m    image = response.content[m
[32m+[m[32m    with open("background.png", "wb") as file:[m
[32m+[m[32m        file.write(image)[m
[32m+[m[32m    print('  Results saved in background.png \n')[m
 [m
 [m
 if __name__ == "__main__":[m
